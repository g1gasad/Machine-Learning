{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3262102,"sourceType":"datasetVersion","datasetId":1976317},{"sourceId":7955894,"sourceType":"datasetVersion","datasetId":4679418}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport cupy as cp","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:56:46.875856Z","iopub.execute_input":"2024-03-28T03:56:46.876610Z","iopub.status.idle":"2024-03-28T03:56:48.829872Z","shell.execute_reply.started":"2024-03-28T03:56:46.876577Z","shell.execute_reply":"2024-03-28T03:56:48.829110Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n# df.head()\n\nfeatures = [col for col in df.columns if col not in ('id', 'kfold', 'target')]\nobject_cols = [col for col in features if 'cat' in col]\nxtest_df = test_df[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df.loc[df.kfold != fold].reset_index(drop=True)\n    xvalid = df.loc[df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n\n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n    # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')    \n    \n    preds_valid = model.predict(xvalid)\n    preds_test = model.predict(xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:56:54.355962Z","iopub.execute_input":"2024-03-28T03:56:54.356334Z","iopub.status.idle":"2024-03-28T03:57:08.273189Z","shell.execute_reply.started":"2024-03-28T03:56:54.356304Z","shell.execute_reply":"2024-03-28T03:57:08.272339Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7245302356488946\nEncoding\nTraining\nFold_2 | RMSE: 0.7241902754354826\nEncoding\nTraining\nFold_3 | RMSE: 0.726511525789112\nEncoding\nTraining\nFold_4 | RMSE: 0.7269540048296184\nEncoding\nTraining\nFold_5 | RMSE: 0.7257602217605351\nMean RMSE: 0.7255892526927286 | Scores STD: 0.0010789829421290003\n","output_type":"stream"}]},{"cell_type":"code","source":"# Standardization\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n# df.head()\n\nfeatures = [col for col in df.columns if col not in ('id', 'kfold', 'target')]\nobject_cols = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith('cont')]\n\nxtest_df = test_df[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df.loc[df.kfold != fold].reset_index(drop=True)\n    xvalid = df.loc[df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n\n    scaler = preprocessing.StandardScaler()    \n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n      # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')    \n\n    preds_valid = model.predict(xvalid)\n    preds_test = model.predict(xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:56:17.382883Z","iopub.execute_input":"2024-03-28T02:56:17.383791Z","iopub.status.idle":"2024-03-28T02:56:31.323117Z","shell.execute_reply.started":"2024-03-28T02:56:17.383756Z","shell.execute_reply":"2024-03-28T02:56:31.322244Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7245302262120346\nEncoding\nTraining\nFold_2 | RMSE: 0.7241323810345601\nEncoding\nTraining\nFold_3 | RMSE: 0.7267897803225617\nEncoding\nTraining\nFold_4 | RMSE: 0.7268506858678104\nEncoding\nTraining\nFold_5 | RMSE: 0.7262141472591808\nMean RMSE: 0.7257034441392294 | Scores STD: 0.001149068069984286\n","output_type":"stream"}]},{"cell_type":"code","source":"# log transformation\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n# df.head()\n\nfeatures = [col for col in df.columns if col not in ('id', 'kfold', 'target')]\nobject_cols = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith('cont')]\n\nfor col in numerical_cols:\n    df[col] = np.log1p(df[col])\n    test_df[col] = np.log1p(test_df[col])\n\nxtest_df = test_df[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df.loc[df.kfold != fold].reset_index(drop=True)\n    xvalid = df.loc[df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n    # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')    \n\n    preds_valid = model.predict(xvalid)\n    preds_test = model.predict(xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:57:30.643470Z","iopub.execute_input":"2024-03-28T02:57:30.644074Z","iopub.status.idle":"2024-03-28T02:57:44.209310Z","shell.execute_reply.started":"2024-03-28T02:57:30.644041Z","shell.execute_reply":"2024-03-28T02:57:44.207403Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7245294851363442\nEncoding\nTraining\nFold_2 | RMSE: 0.7244413738935425\nEncoding\nTraining\nFold_3 | RMSE: 0.7267363418452818\nEncoding\nTraining\nFold_4 | RMSE: 0.7268505017167104\nEncoding\nTraining\nFold_5 | RMSE: 0.725967857757496\nMean RMSE: 0.725705112069875 | Scores STD: 0.0010414912821805108\n","output_type":"stream"}]},{"cell_type":"code","source":"# polynomial features\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n# df.head()\n\nobject_cols = [col for col in df.columns if 'cat' in col]\nnumerical_cols = [col for col in df.columns if col.startswith('cont')]\n\npoly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(test_df[numerical_cols])\ndf_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\nfinal_df = pd.concat([df_poly, df.drop(numerical_cols, axis=1)], axis=1)\nfeatures = [col for col in final_df.columns if col not in ('id', 'kfold', 'target')] # contains object + poly columns\nxtest_df = pd.concat([df_test_poly, test_df.drop(numerical_cols, axis=1)], axis=1)[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = final_df.loc[final_df.kfold != fold].reset_index(drop=True)\n    xvalid = final_df.loc[final_df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n    # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')    \n\n    preds_valid = model.predict(xvalid)\n    preds_test = model.predict(xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:38:35.073589Z","iopub.execute_input":"2024-03-28T03:38:35.074323Z","iopub.status.idle":"2024-03-28T03:39:27.442829Z","shell.execute_reply.started":"2024-03-28T03:38:35.074291Z","shell.execute_reply":"2024-03-28T03:39:27.441712Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7284822358101676\nEncoding\nTraining\nFold_2 | RMSE: 0.7282244135361841\nEncoding\nTraining\nFold_3 | RMSE: 0.730583235442689\nEncoding\nTraining\nFold_4 | RMSE: 0.7290808787512799\nEncoding\nTraining\nFold_5 | RMSE: 0.7300971300182351\nMean RMSE: 0.7292935787117111 | Scores STD: 0.0009116514614242901\n","output_type":"stream"}]},{"cell_type":"code","source":"# OneHotEncoder\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n# df.head()\n\nfeatures = [col for col in df.columns if col not in ('id', 'kfold', 'target')]\nobject_cols = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith('cont')]\nxtest_df = test_df[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df.loc[df.kfold != fold].reset_index(drop=True)\n    xvalid = df.loc[df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    onehot = preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    xtrain_ohe = onehot.fit_transform(xtrain[object_cols])\n    xvalid_ohe = onehot.transform(xvalid[object_cols])\n    xtest_ohe = onehot.transform(xtest[object_cols])\n\n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n\n    num_xtrain = xtrain[numerical_cols]\n    num_xvalid = xvalid[numerical_cols]\n    num_xtest = xtest[numerical_cols]\n\n    final_xtrain = pd.concat([num_xtrain, xtrain_ohe], axis=1)\n    final_xvalid = pd.concat([num_xvalid, xvalid_ohe], axis=1)\n    final_xtest = pd.concat([num_xtest, xtest_ohe], axis=1)\n\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n      # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(final_xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')  # Temporarily switch to CPU for prediction\n        \n    preds_valid = model.predict(final_xvalid)\n    preds_test = model.predict(final_xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:15:39.090144Z","iopub.execute_input":"2024-03-28T03:15:39.090841Z","iopub.status.idle":"2024-03-28T03:15:57.252447Z","shell.execute_reply.started":"2024-03-28T03:15:39.090807Z","shell.execute_reply":"2024-03-28T03:15:57.251564Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7240222419972228\nEncoding\nTraining\nFold_2 | RMSE: 0.7244751419373514\nEncoding\nTraining\nFold_3 | RMSE: 0.7268093267896698\nEncoding\nTraining\nFold_4 | RMSE: 0.7270873671980351\nEncoding\nTraining\nFold_5 | RMSE: 0.7258384152651467\nMean RMSE: 0.7256464986374851 | Scores STD: 0.0012227337947936345\n","output_type":"stream"}]},{"cell_type":"code","source":"# OneHotEncoder + Polynomial Features\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n\nobject_cols = [col for col in df.columns if 'cat' in col]\nnumerical_cols = [col for col in df.columns if col.startswith('cont')]\n\npoly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(test_df[numerical_cols])\ndf_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\nfinal_df = pd.concat([df_poly, df.drop(numerical_cols, axis=1)], axis=1)\nfeatures = [col for col in final_df.columns if col not in ('id', 'kfold', 'target')] # contains object + poly columns\nxtest_df = pd.concat([df_test_poly, test_df.drop(numerical_cols, axis=1)], axis=1)[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = final_df.loc[final_df.kfold != fold].reset_index(drop=True)\n    xvalid = final_df.loc[final_df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    onehot = preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    xtrain_ohe = onehot.fit_transform(xtrain[object_cols])\n    xvalid_ohe = onehot.transform(xvalid[object_cols])\n    xtest_ohe = onehot.transform(xtest[object_cols])\n\n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n\n    final_xtrain = pd.concat([xtrain.drop(object_cols, axis=1), xtrain_ohe], axis=1)\n    final_xvalid = pd.concat([xvalid.drop(object_cols, axis=1), xvalid_ohe], axis=1)\n    final_xtest = pd.concat([xtest.drop(object_cols, axis=1), xtest_ohe], axis=1)\n\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n      # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(final_xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')  # Temporarily switch to CPU for prediction\n        \n    preds_valid = model.predict(final_xvalid)\n    preds_test = model.predict(final_xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:39:31.644863Z","iopub.execute_input":"2024-03-28T03:39:31.645710Z","iopub.status.idle":"2024-03-28T03:40:34.857750Z","shell.execute_reply.started":"2024-03-28T03:39:31.645680Z","shell.execute_reply":"2024-03-28T03:40:34.856965Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7290120913100917\nEncoding\nTraining\nFold_2 | RMSE: 0.7288331308135844\nEncoding\nTraining\nFold_3 | RMSE: 0.7308963909587015\nEncoding\nTraining\nFold_4 | RMSE: 0.7300750952652681\nEncoding\nTraining\nFold_5 | RMSE: 0.7297792518653154\nMean RMSE: 0.7297191920425923 | Scores STD: 0.0007484874777521981\n","output_type":"stream"}]},{"cell_type":"code","source":"# Standardization on (OneHotEncoder + Polynomial Features)\ntest_df = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\")\ndf = pd.read_csv(\"/kaggle/input/30-days-of-ml-5-folds/train_folds.csv\")\n\nobject_cols = [col for col in df.columns if 'cat' in col]\nnumerical_cols = [col for col in df.columns if col.startswith('cont')]\n\npoly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(test_df[numerical_cols])\ndf_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\nfinal_df = pd.concat([df_poly, df.drop(numerical_cols, axis=1)], axis=1)\nfeatures = [col for col in final_df.columns if col not in ('id', 'kfold', 'target')] # contains object + poly columns\nxtest_df = pd.concat([df_test_poly, test_df.drop(numerical_cols, axis=1)], axis=1)[features].copy()\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = final_df.loc[final_df.kfold != fold].reset_index(drop=True)\n    xvalid = final_df.loc[final_df.kfold == fold].reset_index(drop=True)\n    xtest = xtest_df.copy()\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n\n    print(\"Encoding\")\n    onehot = preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    xtrain_ohe = onehot.fit_transform(xtrain[object_cols])\n    xvalid_ohe = onehot.transform(xvalid[object_cols])\n    xtest_ohe = onehot.transform(xtest[object_cols])\n\n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n\n    final_xtrain = pd.concat([xtrain.drop(object_cols, axis=1), xtrain_ohe], axis=1)\n    final_xvalid = pd.concat([xvalid.drop(object_cols, axis=1), xvalid_ohe], axis=1)\n    final_xtest = pd.concat([xtest.drop(object_cols, axis=1), xtest_ohe], axis=1)\n\n    scaler = preprocessing.StandardScaler()    \n    final_xtrain = scaler.fit_transform(final_xtrain)\n    final_xvalid = scaler.transform(final_xvalid)\n    final_xtest = scaler.transform(final_xtest)\n    \n    print(\"Training\")\n    model = xgb.XGBRegressor(random_state=fold, tree_method='hist', device='cuda')\n      # Load training and validation data onto GPU (if necessary)\n    X_train_gpu = cp.array(final_xtrain)\n    y_train_gpu = cp.array(ytrain)\n    \n    model.fit(X_train_gpu, y_train_gpu)\n      # Handle potential device mismatch for prediction\n    if model.device != 'cpu':\n        model.set_params(device='cpu')  # Temporarily switch to CPU for prediction\n        \n    preds_valid = model.predict(final_xvalid)\n    preds_test = model.predict(final_xtest)\n    final_predictions.append(preds_test)\n    rmse = mean_squared_error(preds_valid, yvalid, squared=False)\n    scores.append(rmse)\n    print(f\"Fold_{fold+1} |\", \"RMSE:\", rmse)\n\nprint(\"Mean RMSE:\", np.mean(scores), \"| Scores STD:\", np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:57:13.053343Z","iopub.execute_input":"2024-03-28T03:57:13.054082Z","iopub.status.idle":"2024-03-28T03:58:26.622284Z","shell.execute_reply.started":"2024-03-28T03:57:13.054046Z","shell.execute_reply":"2024-03-28T03:58:26.621437Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Encoding\nTraining\nFold_1 | RMSE: 0.7290593366745904\nEncoding\nTraining\nFold_2 | RMSE: 0.7288331464245963\nEncoding\nTraining\nFold_3 | RMSE: 0.7308964625433366\nEncoding\nTraining\nFold_4 | RMSE: 0.7299282527219462\nEncoding\nTraining\nFold_5 | RMSE: 0.729337678799418\nMean RMSE: 0.7296109754327775 | Scores STD: 0.0007398859810732748\n","output_type":"stream"}]}]}